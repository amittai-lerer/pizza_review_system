---
title: Pizza Review RAG
emoji: ğŸ•
colorFrom: red
colorTo: yellow
sdk: streamlit
app_file: app.py
pinned: false
---

# ğŸ• Advanced NLP-Powered Pizza Review Analysis System

A sophisticated Natural Language Processing system that leverages Large Language Models (LLMs), vector embeddings, and semantic search to provide intelligent analysis of Israeli pizza restaurant reviews. Built with modern software engineering practices and a clean, production-ready architecture.

---

## ğŸ“Š Project Highlights

- Semantic search across user reviews of Israeli pizzerias
- Intelligent Q&A using LangChain + Ollama + ChromaDB
- Modular architecture with full logging and docker support
- Fully local LLM inference using `llama3` via Ollama

---

## ğŸ” Technologies Used

| Layer       | Tool / Lib                        |
|-------------|-----------------------------------|
| LLM         | [Ollama](https://ollama.com) + LangChain |
| Vector DB   | ChromaDB                          |
| UI          | Streamlit                         |
| Embeddings  | BAAI/bge-small-en-v1.5 (HF)       |
| Logging     | Python logger w/ file output      |
| Packaging   | Docker, Docker Compose            |

---

## ğŸ“Š Folder Structure

```
pizza_review_system/
â”œâ”€â”€ app.py              # Streamlit interface
â”œâ”€â”€ core.py             # Prompt logic, LLM calls, city extraction
â”œâ”€â”€ vector.py           # Vector store loading & query interface
â”œâ”€â”€ logger_config.py    # Logging config shared across modules
â”œâ”€â”€ data/               # Contains review CSV file
â”œâ”€â”€ logs/               # Output logs (app.log, vector.log, etc.)
â”œâ”€â”€ Dockerfile          # Container build instructions
â”œâ”€â”€ docker-compose.yml  # Optional container orchestrator
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ README.md           # You're here.
```

---

## ğŸ’ª Features

### âœ… Smart LLM Prompting
- Rewrites user questions into semantic search queries
- Extracts & normalizes city names ("TLV" â†’ "Tel Aviv")
- Ensures consistent prompt format for reliable LLM use

### âœ… Vector-Based Retrieval
- Fast semantic search via ChromaDB
- Filters by city metadata
- Uses Hugging Face embeddings for similarity

### âœ… Clean UI in Streamlit
- Single input for user queries
- Expandable review results
- Annotated answers with context

### âœ… Logging & Debugging
- File + terminal logging (e.g., `logs/app.log`)
- Detailed trace of rewrite, filter, query, and LLM call

---

## ğŸš€ Setup Instructions

```bash
# Clone the project
git clone https://github.com/amittai-lerer/pizza_review_system.git
cd pizza_review_system

# Set up environment
python -m venv venv
source venv/bin/activate  # (Windows: venv\Scripts\activate)

# Install dependencies
pip install -r requirements.txt
```

### ğŸ«  Run Ollama LLM Locally
```bash
# Make sure you have Ollama installed (https://ollama.com/download)
ollama pull llama3
ollama run llama3
```

### ğŸŒ Run Streamlit App
```bash
streamlit run app.py
```

Open the browser at [http://localhost:8501](http://localhost:8501)

---

## ğŸ’¡ Sample Queries
```text
Best pizza in TLV?
What are top Neapolitan pizza places?
Which pizzeria in Jerusalem has spicy toppings?
Who has the best pizza crust?
```

---

## ğŸŒŸ License

This project is licensed under the MIT License.









--------------






























---
title: Pizza Review RAG
emoji: ğŸ•
colorFrom: red
colorTo: yellow
sdk: streamlit
app_file: app.py
pinned: false
---

# ğŸ• Pizza Review RAG â€“ AI-Powered Pizza Discovery for Israel

A sophisticated Natural Language Processing (NLP) system that leverages Large Language Models (LLMs), vector embeddings, and semantic search to provide intelligent analysis of Israeli pizza restaurant reviews. Built with production-grade engineering practices and a clean, modular architecture.

---

## ğŸ“Š Project Highlights

* ğŸ”„ **LLM Switching**: Use either a **local model** (`llama3` via Ollama) or a **cloud model** (Together AI)
* ğŸ§  **Prompt Rewriting**: Refines user queries to improve search and answer quality
* ğŸ—œï¸ **City Extraction**: Converts slang or abbreviations like `TLV` â†’ `Tel Aviv`
* ğŸ” **Vector Search**: Uses ChromaDB with Hugging Face embeddings for fast semantic retrieval
* ğŸ–¥ï¸ **Modern UI**: Streamlit frontend with logging, query results, and LLM controls
* ğŸ³ **Docker Support**: Easy to containerize for deployment or portability

---

## ğŸ” Technologies Used

| Layer      | Tool / Lib                                                                      |
| ---------- | ------------------------------------------------------------------------------- |
| LLM        | [Ollama](https://ollama.com), [Together AI](https://www.together.ai), LangChain |
| Vector DB  | [ChromaDB](https://www.trychroma.com)                                           |
| Embeddings | [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)         |
| UI         | [Streamlit](https://streamlit.io)                                               |
| Logging    | Python built-in logging module                                                  |
| Container  | Docker, Docker Compose                                                          |

---

## ğŸ“ Folder Structure

```
pizza_review_system/
â”œâ”€â”€ app.py              # Streamlit interface
â”œâ”€â”€ core.py             # Prompt logic, LLM calls, city extraction
â”œâ”€â”€ vector.py           # Vector store loading & query interface
â”œâ”€â”€ logger_config.py    # Logging config shared across modules
â”œâ”€â”€ data/               # Contains review CSV file
â”œâ”€â”€ logs/               # Output logs (app.log, vector.log, etc.)
â”œâ”€â”€ .streamlit/         # Streamlit config + secrets.toml (ignored by Git)
â”œâ”€â”€ Dockerfile          # Container build instructions
â”œâ”€â”€ docker-compose.yml  # Optional container orchestrator
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md           # You're here.
```

---

## ğŸ’ª Key Features

### âœ… Smart LLM Prompting

* Rewrites vague or casual input into semantically structured review prompts
* Example: `pizza in JLM?` â†’ `I had great pizza in Jerusalem.`

### âœ… Semantic Retrieval

* Filters reviews by city and similarity
* Uses BAAI embeddings and ChromaDB for scalable local vector search

### âœ… Flexible LLM Execution

* Use `llama3` locally via Ollama (no API cost)
* Or query Together AI's `Llama-3.3-70B-Instruct-Turbo-Free` in the cloud

### âœ… Rich Logging

* Tracks rewrite steps, LLM calls, parsed results, and user queries
* Logs to both file (`logs/app.log`) and console

---

## ğŸš€ Setup Instructions

### 1. Clone & Install

```bash
git clone https://github.com/amittai-lerer/pizza_review_system.git
cd pizza_review_system
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

2. ğŸ” Add Secrets for Cloud Model (Optional)

To use the Together AI model, create a file at:

.streamlit/secrets.toml

Add your API key:

TOGETHER_API_KEY = "your-together-api-key"

âš ï¸ This file is already excluded from version control via .gitignore.

3. ğŸ§  Run Ollama Locally (Optional)

If you prefer local LLM inference:

ollama pull llama3
ollama run llama3

4. ğŸš€ Launch the App

streamlit run app.py

Open your browser to: http://localhost:8501

ğŸ’¬ Example Queries

Best pizza in TLV?
Spiciest toppings in Haifa?
Authentic Neapolitan pizza in JLM?
Where to find gluten-free pizza in Holon?
Top-rated places for pizza crust?

ğŸ§ª Cloud Model Integration (Optional)

If using Together AI:

Ensure TOGETHER_API_KEY is set in .streamlit/secrets.toml

The app will automatically switch to Together AI when enabled

Default model:meta-llama/Llama-3.3-70B-Instruct-Turbo-Free

ğŸ³ Docker Support (Optional)

To build and run the app in a container:

docker build -t pizza-review .
docker run -p 8501:8501 pizza-review

Or use Docker Compose:

docker-compose up

ğŸ“œ License

MIT License â€” freely usable for educational or portfolio purposes.

